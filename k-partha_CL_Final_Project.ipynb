{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Workbook 2 - CL Final Project.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "8e3430dc246c4f059151385985c8413f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cb726738f0a046f6969cf3ff51a72ad1",
       "IPY_MODEL_02672f2b379449a3900f4b825b5078f1"
      ],
      "layout": "IPY_MODEL_68337f8c90fa4d8d8369729d3fc36d1c"
     }
    },
    "cb726738f0a046f6969cf3ff51a72ad1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d6f68c28c6144c1fa28b2269ec7dac11",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1ead5d53a4da4198a17af56c8971bc39",
      "value": 231508
     }
    },
    "02672f2b379449a3900f4b825b5078f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_da33be0359664928a6735237e2240413",
      "placeholder": "​",
      "style": "IPY_MODEL_a5f21d04f0cc49628ad1c7b6de6c5981",
      "value": " 232k/232k [00:00&lt;00:00, 306kB/s]"
     }
    },
    "68337f8c90fa4d8d8369729d3fc36d1c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d6f68c28c6144c1fa28b2269ec7dac11": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ead5d53a4da4198a17af56c8971bc39": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "da33be0359664928a6735237e2240413": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a5f21d04f0cc49628ad1c7b6de6c5981": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nFkME77f1Zh"
   },
   "source": [
    "# Fitting BERT Classifier to Twitter MBTI"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eTCP80HqWCZs",
    "outputId": "916e43f5-0598-4327-9f7f-d50a9c569658"
   },
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from transformers import AdamW, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6DkVY2HIWRPC",
    "outputId": "4f24f89c-977f-475c-8a4c-0dce3cf85715",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 GPU(s) available.\n",
      "We will use the GPU: TITAN X (Pascal)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def expand_frame(df, length):\n",
    "    ndf = pd.DataFrame()\n",
    "\n",
    "    for i in range(1,30):\n",
    "        adf = df.copy(deep= True)\n",
    "        adf['text'] = adf['text'].apply(lambda x: x[0 + length*5*i : length * 5 * (i+1)])\n",
    "        ndf = ndf.append(adf)\n",
    "        ndf.reset_index(drop=True, inplace= True)\n",
    "    return ndf\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPbf8Yvyf-Hn"
   },
   "source": [
    "# Loading the Twitter personality dataset:\n",
    "\n",
    "# OPTION 1 : Load raw without tokenized:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mt2B4sVCsZx7"
   },
   "source": [
    "ofile = open('Data/personality_likes_large.csv', encoding = 'cp1252', mode='r' )\n",
    "raw_df = pd.read_csv(ofile ,index_col=0)\n",
    "ofile.close()"
   ],
   "execution_count": 41,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wifile = open('train_tokenized_large_exp.csv', mode = 'w+')\n",
    "wtfile = open('test_tokenized_large_exp.csv', mode = 'w+')\n",
    "\n",
    "wifile.truncate(0)\n",
    "wtfile.truncate(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(raw_df, stratify = raw_df['type'], random_state= 1729, test_size= 0.12)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df = expand_frame(train_df, 256)\n",
    "\n",
    "train_df.dropna(inplace = True)\n",
    "test_df.to_csv(wtfile)\n",
    "\n",
    "wtfile.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "outputs": [
    {
     "data": {
      "text/plain": "'[CLS] 7 @brad_polumbo @mattyglesias Not just can but can\\'t create money out of anything else@MyPillowUSA what’s good?@leftwinggal Nobody is marginalizing Dutch-Americans what the fuckhttps://t.co/OQTASiMt5u@AndrewYang https://t.co/HfvynOpeHu@AndrewYang https://t.co/suSZXLoK2QSo basically, only rich while men can try to make a $, the rest of you are not worthy. OK.  https://t.co/CrQ6q9i40YWho’s this Fucking Guy... $GME $AMC to the  MOON  https://t.co/KUmZPWdO9l@reddittrading I have a romantic relationship with my $AMC stock.Baptist leaders: \"Only men can be in leadership positions, so they can use their strength to protect women.\"\\n\\nAlso Baptist leaders: \"I\\'m sorry, I can\\'t do anything substantial to help sexual abuse victims because other men might say mean things about me.\"\\n\\nA plague of cowards.Democratic leadership laying down the gauntlet on Marjorie Taylor Greene: Steny Hoyer is expected to tell Kevin McCarthy he has 72 hours to strip Marjorie Taylor Greene of her committee assignments or Democrats will bring the issue to the House floor, a source familiar tells me.Stop saying history will judge them.\\n\\nJudge them now.  With judges.Im #notbuyingsilver. Fake news.\\n\\n#GMEtothemoon #gme #GMEGANGSilver is a distraction. Full stopThe United States is still in the middle [SEP]'"
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create sentence and label lists\n",
    "sentences = train_df.text.values\n",
    "\n",
    "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
    "sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in sentences]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize the first sentence:\n",
      "['[CLS]', '@', 'mig', '##non', '##10', '##44', '@', 'mig', '##non', '##10', '##44', '!', '!', '@', 'mig', '##non', '##10', '##44', '@', 'mig', '##non', '##10', '##44', '?', '@', 'mig', '##non', '##10', '##44', '@', 'mig', '##non', '##10', '##44', '.', '.', '.', '.', '@', 'mig', '##non', '##10', '##44', '@', 'mig', '##non', '##10', '##44', '~', '~', '~', '@', 'mig', '##non', '##10', '##44', '.', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "train_df['tokenized_texts'] = tokenized_texts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "outputs": [],
   "source": [
    "train_df.to_csv(wifile)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "outputs": [
    {
     "data": {
      "text/plain": "                   liked_by  \\\n0       1281267858639486976   \n1                  17724827   \n2                  84772941   \n3                3416461845   \n4                3075434919   \n...                     ...   \n106802           2356621515   \n106803   706889633670557697   \n106804             91546751   \n106805             24946566   \n106806            218585083   \n\n                                                     text  type  extravert  \\\n0        @mignon1044  @mignon1044 !!@mignon1044 @migno...  ISTJ          0   \n1       an sebegitu menyebabkan anda cuba mengganggu d...  INFJ          0   \n2        that I can use strong language.the news: stay...  ENFP          1   \n3       s://t.co/JZXYOUsCqoSelena Gomez's Makeup Artis...  ESFP          1   \n4       de papai, queria eu ser, único erro dele foi r...  ESFP          1   \n...                                                   ...   ...        ...   \n106802                                                NaN  ESFP          1   \n106803                                                NaN  INFJ          0   \n106804  @LiviaBellona @TheKimClub @PharmDame @PeterFMa...  ENFJ          1   \n106805  questioned my non-linear liberal arts path. ht...  INTJ          0   \n106806  something about it. No one else can do it for ...  INTJ          0   \n\n        intuitive  thinking  judging  NT  SF  NF  ST  NJ  NP  SJ  SP  \\\n0               0         1        1   0   0   0   1   0   0   1   0   \n1               1         0        1   0   0   1   0   1   0   0   0   \n2               1         0        0   0   0   1   0   0   1   0   0   \n3               0         0        0   0   1   0   0   0   0   0   1   \n4               0         0        0   0   1   0   0   0   0   0   1   \n...           ...       ...      ...  ..  ..  ..  ..  ..  ..  ..  ..   \n106802          0         0        0   0   1   0   0   0   0   0   1   \n106803          1         0        1   0   0   1   0   1   0   0   0   \n106804          1         0        1   0   0   1   0   1   0   0   0   \n106805          1         1        1   1   0   0   0   1   0   0   0   \n106806          1         1        1   1   0   0   0   1   0   0   0   \n\n                                          tokenized_texts  \n0       ['[CLS]', '@', 'mig', '##non', '##10', '##44',...  \n1       ['[CLS]', 'an', 'se', '##be', '##git', '##u', ...  \n2       ['[CLS]', 'that', 'i', 'can', 'use', 'strong',...  \n3       ['[CLS]', 's', ':', '/', '/', 't', '.', 'co', ...  \n4       ['[CLS]', 'de', 'papa', '##i', ',', 'que', '##...  \n...                                                   ...  \n106802                                 ['[CLS]', '[SEP]']  \n106803                                 ['[CLS]', '[SEP]']  \n106804  ['[CLS]', '@', 'liv', '##ia', '##bell', '##ona...  \n106805  ['[CLS]', 'questioned', 'my', 'non', '-', 'lin...  \n106806  ['[CLS]', 'something', 'about', 'it', '.', 'no...  \n\n[106807 rows x 16 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>liked_by</th>\n      <th>text</th>\n      <th>type</th>\n      <th>extravert</th>\n      <th>intuitive</th>\n      <th>thinking</th>\n      <th>judging</th>\n      <th>NT</th>\n      <th>SF</th>\n      <th>NF</th>\n      <th>ST</th>\n      <th>NJ</th>\n      <th>NP</th>\n      <th>SJ</th>\n      <th>SP</th>\n      <th>tokenized_texts</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1281267858639486976</td>\n      <td>@mignon1044  @mignon1044 !!@mignon1044 @migno...</td>\n      <td>ISTJ</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>['[CLS]', '@', 'mig', '##non', '##10', '##44',...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>17724827</td>\n      <td>an sebegitu menyebabkan anda cuba mengganggu d...</td>\n      <td>INFJ</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['[CLS]', 'an', 'se', '##be', '##git', '##u', ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>84772941</td>\n      <td>that I can use strong language.the news: stay...</td>\n      <td>ENFP</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['[CLS]', 'that', 'i', 'can', 'use', 'strong',...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3416461845</td>\n      <td>s://t.co/JZXYOUsCqoSelena Gomez's Makeup Artis...</td>\n      <td>ESFP</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>['[CLS]', 's', ':', '/', '/', 't', '.', 'co', ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3075434919</td>\n      <td>de papai, queria eu ser, único erro dele foi r...</td>\n      <td>ESFP</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>['[CLS]', 'de', 'papa', '##i', ',', 'que', '##...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>106802</th>\n      <td>2356621515</td>\n      <td>NaN</td>\n      <td>ESFP</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>['[CLS]', '[SEP]']</td>\n    </tr>\n    <tr>\n      <th>106803</th>\n      <td>706889633670557697</td>\n      <td>NaN</td>\n      <td>INFJ</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['[CLS]', '[SEP]']</td>\n    </tr>\n    <tr>\n      <th>106804</th>\n      <td>91546751</td>\n      <td>@LiviaBellona @TheKimClub @PharmDame @PeterFMa...</td>\n      <td>ENFJ</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['[CLS]', '@', 'liv', '##ia', '##bell', '##ona...</td>\n    </tr>\n    <tr>\n      <th>106805</th>\n      <td>24946566</td>\n      <td>questioned my non-linear liberal arts path. ht...</td>\n      <td>INTJ</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['[CLS]', 'questioned', 'my', 'non', '-', 'lin...</td>\n    </tr>\n    <tr>\n      <th>106806</th>\n      <td>218585083</td>\n      <td>something about it. No one else can do it for ...</td>\n      <td>INTJ</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['[CLS]', 'something', 'about', 'it', '.', 'no...</td>\n    </tr>\n  </tbody>\n</table>\n<p>106807 rows × 16 columns</p>\n</div>"
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wifile.close()\n",
    "ofile = open('train_tokenized_large_exp.csv',  mode='r' )\n",
    "edf = pd.read_csv(ofile, index_col = 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# OPTION 2: Load Dataframe from disk"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "test_file = open('test_tokenized_large_exp.csv')\n",
    "ofile = open('train_tokenized_large_exp.csv')\n",
    "\n",
    "edf = pd.read_csv(ofile, index_col = 0)\n",
    "\n",
    "edf.dropna(inplace= True)\n",
    "#edf, test_df = train_test_split(edf, random_state=2020, test_size=0.10)\n",
    "test_df = pd.read_csv(test_file, index_col=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "test_df = test_df.reset_index(drop = True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "def fourType(x):\n",
    "    if ((x=='INFJ') | (x=='INFP') | (x=='ENFJ') | (x=='ENFP')):\n",
    "        return 1\n",
    "    elif ((x=='INTJ') | (x=='INTP') | (x=='ENTJ') | (x=='ENTP')):\n",
    "        return 2\n",
    "    elif ((x=='ISFJ') | (x=='ISTJ') | (x=='ESFJ') | (x=='ESTJ')):\n",
    "        return 3\n",
    "    else:\n",
    "        return 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "edf['role'] = edf['type'].map(fourType)\n",
    "test_df['role'] = test_df['type'].map(fourType)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "                   liked_by  \\\n105039             15620292   \n52013            3403218160   \n57263   1244901512700030976   \n71092              26871926   \n96461               6046132   \n...                     ...   \n22249             172118801   \n58010             491669361   \n78902              26541394   \n9532     790271395855003649   \n46040    880858425680330752   \n\n                                                     text  type  extravert  \\\n105039  rUryJJaqPnokay I know you guys like TTRPGs.  h...  INFP          0   \n52013   CiBSdFW2vSreal pokemon merchandise https://t.c...  INFP          0   \n57263   it out for Free (link in bio)\\n#brunette #curl...  INFJ          0   \n71092   l me anh(at)https://t.co/Leb3wLQo3x.We just wa...  ENFJ          1   \n96461   ng it. Anyway the commercially-successful-but-...  INFP          0   \n...                                                   ...   ...        ...   \n22249   MUNICADO https://t.co/QTXWaFw6Oyhttps://t.co/A...  ESFP          1   \n58010   me ambassador for @worldvisioncan. Now we know...  ESFP          1   \n78902   the developers specifically had CRT monitors i...  ISTP          0   \n9532    ed before my brains melt.  #jiroukyouka #jiro ...  ISFP          0   \n46040   nt to call him Nines, it's totally okay. https...  ISTP          0   \n\n        intuitive  thinking  judging  NT  SF  NF  ST  NJ  NP  SJ  SP  \\\n105039          1         0        0   0   0   1   0   0   1   0   0   \n52013           1         0        0   0   0   1   0   0   1   0   0   \n57263           1         0        1   0   0   1   0   1   0   0   0   \n71092           1         0        1   0   0   1   0   1   0   0   0   \n96461           1         0        0   0   0   1   0   0   1   0   0   \n...           ...       ...      ...  ..  ..  ..  ..  ..  ..  ..  ..   \n22249           0         0        0   0   1   0   0   0   0   0   1   \n58010           0         0        0   0   1   0   0   0   0   0   1   \n78902           0         1        0   0   0   0   1   0   0   0   1   \n9532            0         0        0   0   1   0   0   0   0   0   1   \n46040           0         1        0   0   0   0   1   0   0   0   1   \n\n                                          tokenized_texts  role  \n105039  ['[CLS]', 'ru', '##ry', '##j', '##ja', '##q', ...     1  \n52013   ['[CLS]', 'ci', '##bs', '##df', '##w', '##2', ...     1  \n57263   ['[CLS]', 'it', 'out', 'for', 'free', '(', 'li...     1  \n71092   ['[CLS]', 'l', 'me', 'an', '##h', '(', 'at', '...     1  \n96461   ['[CLS]', 'ng', 'it', '.', 'anyway', 'the', 'c...     1  \n...                                                   ...   ...  \n22249   ['[CLS]', 'mu', '##nica', '##do', 'https', ':'...     0  \n58010   ['[CLS]', 'me', 'ambassador', 'for', '@', 'wor...     0  \n78902   ['[CLS]', 'the', 'developers', 'specifically',...     0  \n9532    ['[CLS]', 'ed', 'before', 'my', 'brains', 'mel...     0  \n46040   ['[CLS]', 'nt', 'to', 'call', 'him', 'nine', '...     0  \n\n[43232 rows x 17 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>liked_by</th>\n      <th>text</th>\n      <th>type</th>\n      <th>extravert</th>\n      <th>intuitive</th>\n      <th>thinking</th>\n      <th>judging</th>\n      <th>NT</th>\n      <th>SF</th>\n      <th>NF</th>\n      <th>ST</th>\n      <th>NJ</th>\n      <th>NP</th>\n      <th>SJ</th>\n      <th>SP</th>\n      <th>tokenized_texts</th>\n      <th>role</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>105039</th>\n      <td>15620292</td>\n      <td>rUryJJaqPnokay I know you guys like TTRPGs.  h...</td>\n      <td>INFP</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['[CLS]', 'ru', '##ry', '##j', '##ja', '##q', ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>52013</th>\n      <td>3403218160</td>\n      <td>CiBSdFW2vSreal pokemon merchandise https://t.c...</td>\n      <td>INFP</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['[CLS]', 'ci', '##bs', '##df', '##w', '##2', ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>57263</th>\n      <td>1244901512700030976</td>\n      <td>it out for Free (link in bio)\\n#brunette #curl...</td>\n      <td>INFJ</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['[CLS]', 'it', 'out', 'for', 'free', '(', 'li...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>71092</th>\n      <td>26871926</td>\n      <td>l me anh(at)https://t.co/Leb3wLQo3x.We just wa...</td>\n      <td>ENFJ</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['[CLS]', 'l', 'me', 'an', '##h', '(', 'at', '...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>96461</th>\n      <td>6046132</td>\n      <td>ng it. Anyway the commercially-successful-but-...</td>\n      <td>INFP</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['[CLS]', 'ng', 'it', '.', 'anyway', 'the', 'c...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>22249</th>\n      <td>172118801</td>\n      <td>MUNICADO https://t.co/QTXWaFw6Oyhttps://t.co/A...</td>\n      <td>ESFP</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>['[CLS]', 'mu', '##nica', '##do', 'https', ':'...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>58010</th>\n      <td>491669361</td>\n      <td>me ambassador for @worldvisioncan. Now we know...</td>\n      <td>ESFP</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>['[CLS]', 'me', 'ambassador', 'for', '@', 'wor...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>78902</th>\n      <td>26541394</td>\n      <td>the developers specifically had CRT monitors i...</td>\n      <td>ISTP</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>['[CLS]', 'the', 'developers', 'specifically',...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9532</th>\n      <td>790271395855003649</td>\n      <td>ed before my brains melt.  #jiroukyouka #jiro ...</td>\n      <td>ISFP</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>['[CLS]', 'ed', 'before', 'my', 'brains', 'mel...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>46040</th>\n      <td>880858425680330752</td>\n      <td>nt to call him Nines, it's totally okay. https...</td>\n      <td>ISTP</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>['[CLS]', 'nt', 'to', 'call', 'him', 'nine', '...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>43232 rows × 17 columns</p>\n</div>"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = edf[edf['role']==1].sample(10808, random_state = 34)\n",
    "df = df.append(edf[edf['role']==2].sample(10808, random_state = 35))\n",
    "df = df.append(edf[edf['role']==3].sample(10808, random_state = 35))\n",
    "df = df.append(edf[edf['role']==0].sample(10808, random_state = 35))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ttNyQXn0V4ba"
   },
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "tokenized_texts = df['tokenized_texts'].map(literal_eval)\n",
    "\n",
    "test_df = test_df.reset_index(drop = True)"
   ],
   "execution_count": 62,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "0        [[CLS], er, ##y, despite, l, ##w, ##j, ', s, w...\n1        [[CLS], ph, #, gu, ##hit, ##pina, ##s, https, ...\n2        [[CLS], h, eli, ##mina, ##das, da, pro, ##va, ...\n3        [[CLS], p, ;, se, ##m, o, bo, ##a, no, ##ite, ...\n4        [[CLS], for, an, interview, with, @, maggie, _...\n                               ...                        \n43227    [[CLS], https, :, /, /, t, ., co, /, ct, ##ek,...\n43228    [[CLS], yang, ku, ##rang, pen, ##ting, ., http...\n43229    [[CLS], tis, i, ’, ve, never, had, a, pizza, w...\n43230    [[CLS], r, lists, !, !, !, !, thank, you, all,...\n43231    [[CLS], s, :, /, /, t, ., co, /, f, ##s, ##x, ...\nName: tokenized_texts, Length: 43232, dtype: object"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_texts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wywdbBISguFj"
   },
   "source": [
    "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
    "# In the original paper, the authors used a length of 512.\n",
    "MAX_LEN = 256"
   ],
   "execution_count": 76,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ko6fHIlAiHm7"
   },
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ],
   "execution_count": 77,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VSaU7DbkgzWh"
   },
   "source": [
    "# Pad our input tokens\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ],
   "execution_count": 78,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[  101,  9413,  2100, ...,  2860,  3501,  5176],\n       [  101,  6887,  1001, ..., 16770,  1024,  1013],\n       [  101,  1044, 12005, ...,  4487,  1012,  1001],\n       ...,\n       [  101, 22320,  1045, ...,  1996, 11675,  1012],\n       [  101,  1054,  7201, ...,  2000,  2079,  2007],\n       [  101,  1055,  1024, ..., 18351,  5622,  2912]])"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FEUBxUm8g0ov"
   },
   "source": [
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)"
   ],
   "execution_count": 80,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "A4QPyBRDiIx1"
   },
   "source": [
    "# Use train_test_split to split our data into train and validation sets for training\n",
    "\n",
    "labels = df.role.values\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
    "                                                            random_state=2020, test_size=0.01)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2020, test_size=0.01)"
   ],
   "execution_count": 81,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h6muAlDeiac7"
   },
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "UJDljt39iddQ"
   },
   "source": [
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ],
   "execution_count": 83,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-83-46ab3fc4a055>:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_inputs = torch.tensor(train_inputs)\n",
      "<ipython-input-83-46ab3fc4a055>:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  validation_inputs = torch.tensor(validation_inputs)\n",
      "<ipython-input-83-46ab3fc4a055>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_labels = torch.tensor(train_labels)\n",
      "<ipython-input-83-46ab3fc4a055>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  validation_labels = torch.tensor(validation_labels)\n",
      "<ipython-input-83-46ab3fc4a055>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_masks = torch.tensor(train_masks)\n",
      "<ipython-input-83-46ab3fc4a055>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  validation_masks = torch.tensor(validation_masks)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CgmJ-E_VigTt"
   },
   "source": [
    "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
    "batch_size = 8\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ],
   "execution_count": 85,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "id": "IYkR2tJxiiJk",
    "outputId": "ef731b32-df90-4f9b-eea5-02c6c926c4e0"
   },
   "source": [
    "from torch import nn\n",
    "from transformers import XLNetTokenizer, XLNetForSequenceClassification\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "from transformers import XLNetTokenizer, XLNetForSequenceClassification\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = nn.DataParallel(model, device_ids=[0, 1])\n",
    "model.cuda()"
   ],
   "execution_count": 86,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "DataParallel(\n  (module): BertForSequenceClassification(\n    (bert): BertModel(\n      (embeddings): BertEmbeddings(\n        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n        (position_embeddings): Embedding(512, 768)\n        (token_type_embeddings): Embedding(2, 768)\n        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (encoder): BertEncoder(\n        (layer): ModuleList(\n          (0): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (1): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (2): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (3): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (4): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (5): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (6): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (7): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (8): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (9): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (10): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (11): BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (pooler): BertPooler(\n        (dense): Linear(in_features=768, out_features=768, bias=True)\n        (activation): Tanh()\n      )\n    )\n    (dropout): Dropout(p=0.1, inplace=False)\n    (classifier): Linear(in_features=768, out_features=4, bias=True)\n  )\n)"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mfwPyoh9ii0x"
   },
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n"
   ],
   "execution_count": 87,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "likqi1-FimRZ"
   },
   "source": [
    "# This variable contains all of the hyperparemeter information our training loop needs\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 1e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n"
   ],
   "execution_count": 88,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JJcXKErgioEE"
   },
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ],
   "execution_count": 89,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "j-fvXJW_ip1k"
   },
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ],
   "execution_count": 90,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LtCJt0A0irEL"
   },
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ],
   "execution_count": 91,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "wnwi-bj1itbL",
    "outputId": "a53d9a75-d85c-4387-f9c8-6712e8853ccb"
   },
   "source": [
    "import random\n",
    "torch.cuda.empty_cache()\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 44\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Store the average loss after each epoch so we can plot them.\n",
    "loss_values = []\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # This will return the loss (rather than the model output) because we\n",
    "        # have provided the `labels`.\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        outputs = model(b_input_ids, \n",
    "                    token_type_ids=None, \n",
    "                    attention_mask=b_input_mask, \n",
    "                    labels=b_labels)\n",
    "        \n",
    "        # The call to `model` always returns a tuple, so we need to pull the \n",
    "        # loss value out of the tuple.\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        loss = loss.mean()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # Telling the model not to compute or store gradients, saving memory and\n",
    "        # speeding up validation\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # This will return the logits rather than the loss because we have\n",
    "            # not provided labels.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "        \n",
    "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "        # values prior to applying an activation function like the softmax.\n",
    "        logits = outputs[0]\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        # Accumulate the total accuracy.\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        # Track the number of batches\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ],
   "execution_count": 92,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  5,350.    Elapsed: 0:00:14.\n",
      "  Batch    80  of  5,350.    Elapsed: 0:00:26.\n",
      "  Batch   120  of  5,350.    Elapsed: 0:00:38.\n",
      "  Batch   160  of  5,350.    Elapsed: 0:00:52.\n",
      "  Batch   200  of  5,350.    Elapsed: 0:01:04.\n",
      "  Batch   240  of  5,350.    Elapsed: 0:01:16.\n",
      "  Batch   280  of  5,350.    Elapsed: 0:01:28.\n",
      "  Batch   320  of  5,350.    Elapsed: 0:01:40.\n",
      "  Batch   360  of  5,350.    Elapsed: 0:01:52.\n",
      "  Batch   400  of  5,350.    Elapsed: 0:02:04.\n",
      "  Batch   440  of  5,350.    Elapsed: 0:02:16.\n",
      "  Batch   480  of  5,350.    Elapsed: 0:02:29.\n",
      "  Batch   520  of  5,350.    Elapsed: 0:02:41.\n",
      "  Batch   560  of  5,350.    Elapsed: 0:02:53.\n",
      "  Batch   600  of  5,350.    Elapsed: 0:03:07.\n",
      "  Batch   640  of  5,350.    Elapsed: 0:03:19.\n",
      "  Batch   680  of  5,350.    Elapsed: 0:03:31.\n",
      "  Batch   720  of  5,350.    Elapsed: 0:03:43.\n",
      "  Batch   760  of  5,350.    Elapsed: 0:03:55.\n",
      "  Batch   800  of  5,350.    Elapsed: 0:04:07.\n",
      "  Batch   840  of  5,350.    Elapsed: 0:04:21.\n",
      "  Batch   880  of  5,350.    Elapsed: 0:04:33.\n",
      "  Batch   920  of  5,350.    Elapsed: 0:04:45.\n",
      "  Batch   960  of  5,350.    Elapsed: 0:04:57.\n",
      "  Batch 1,000  of  5,350.    Elapsed: 0:05:11.\n",
      "  Batch 1,040  of  5,350.    Elapsed: 0:05:23.\n",
      "  Batch 1,080  of  5,350.    Elapsed: 0:05:35.\n",
      "  Batch 1,120  of  5,350.    Elapsed: 0:05:48.\n",
      "  Batch 1,160  of  5,350.    Elapsed: 0:06:00.\n",
      "  Batch 1,200  of  5,350.    Elapsed: 0:06:13.\n",
      "  Batch 1,240  of  5,350.    Elapsed: 0:06:25.\n",
      "  Batch 1,280  of  5,350.    Elapsed: 0:06:37.\n",
      "  Batch 1,320  of  5,350.    Elapsed: 0:06:49.\n",
      "  Batch 1,360  of  5,350.    Elapsed: 0:07:01.\n",
      "  Batch 1,400  of  5,350.    Elapsed: 0:07:13.\n",
      "  Batch 1,440  of  5,350.    Elapsed: 0:07:26.\n",
      "  Batch 1,480  of  5,350.    Elapsed: 0:07:38.\n",
      "  Batch 1,520  of  5,350.    Elapsed: 0:07:50.\n",
      "  Batch 1,560  of  5,350.    Elapsed: 0:08:04.\n",
      "  Batch 1,600  of  5,350.    Elapsed: 0:08:16.\n",
      "  Batch 1,640  of  5,350.    Elapsed: 0:08:28.\n",
      "  Batch 1,680  of  5,350.    Elapsed: 0:08:40.\n",
      "  Batch 1,720  of  5,350.    Elapsed: 0:08:52.\n",
      "  Batch 1,760  of  5,350.    Elapsed: 0:09:06.\n",
      "  Batch 1,800  of  5,350.    Elapsed: 0:09:18.\n",
      "  Batch 1,840  of  5,350.    Elapsed: 0:09:30.\n",
      "  Batch 1,880  of  5,350.    Elapsed: 0:09:43.\n",
      "  Batch 1,920  of  5,350.    Elapsed: 0:09:56.\n",
      "  Batch 1,960  of  5,350.    Elapsed: 0:10:08.\n",
      "  Batch 2,000  of  5,350.    Elapsed: 0:10:20.\n",
      "  Batch 2,040  of  5,350.    Elapsed: 0:10:34.\n",
      "  Batch 2,080  of  5,350.    Elapsed: 0:10:46.\n",
      "  Batch 2,120  of  5,350.    Elapsed: 0:10:58.\n",
      "  Batch 2,160  of  5,350.    Elapsed: 0:11:10.\n",
      "  Batch 2,200  of  5,350.    Elapsed: 0:11:23.\n",
      "  Batch 2,240  of  5,350.    Elapsed: 0:11:35.\n",
      "  Batch 2,280  of  5,350.    Elapsed: 0:11:47.\n",
      "  Batch 2,320  of  5,350.    Elapsed: 0:11:59.\n",
      "  Batch 2,360  of  5,350.    Elapsed: 0:12:12.\n",
      "  Batch 2,400  of  5,350.    Elapsed: 0:12:24.\n",
      "  Batch 2,440  of  5,350.    Elapsed: 0:12:36.\n",
      "  Batch 2,480  of  5,350.    Elapsed: 0:12:50.\n",
      "  Batch 2,520  of  5,350.    Elapsed: 0:13:02.\n",
      "  Batch 2,560  of  5,350.    Elapsed: 0:13:14.\n",
      "  Batch 2,600  of  5,350.    Elapsed: 0:13:27.\n",
      "  Batch 2,640  of  5,350.    Elapsed: 0:13:39.\n",
      "  Batch 2,680  of  5,350.    Elapsed: 0:13:51.\n",
      "  Batch 2,720  of  5,350.    Elapsed: 0:14:05.\n",
      "  Batch 2,760  of  5,350.    Elapsed: 0:14:17.\n",
      "  Batch 2,800  of  5,350.    Elapsed: 0:14:29.\n",
      "  Batch 2,840  of  5,350.    Elapsed: 0:14:42.\n",
      "  Batch 2,880  of  5,350.    Elapsed: 0:14:55.\n",
      "  Batch 2,920  of  5,350.    Elapsed: 0:15:07.\n",
      "  Batch 2,960  of  5,350.    Elapsed: 0:15:21.\n",
      "  Batch 3,000  of  5,350.    Elapsed: 0:15:33.\n",
      "  Batch 3,040  of  5,350.    Elapsed: 0:15:45.\n",
      "  Batch 3,080  of  5,350.    Elapsed: 0:15:57.\n",
      "  Batch 3,120  of  5,350.    Elapsed: 0:16:10.\n",
      "  Batch 3,160  of  5,350.    Elapsed: 0:16:22.\n",
      "  Batch 3,200  of  5,350.    Elapsed: 0:16:34.\n",
      "  Batch 3,240  of  5,350.    Elapsed: 0:16:46.\n",
      "  Batch 3,280  of  5,350.    Elapsed: 0:16:59.\n",
      "  Batch 3,320  of  5,350.    Elapsed: 0:17:11.\n",
      "  Batch 3,360  of  5,350.    Elapsed: 0:17:23.\n",
      "  Batch 3,400  of  5,350.    Elapsed: 0:17:37.\n",
      "  Batch 3,440  of  5,350.    Elapsed: 0:17:49.\n",
      "  Batch 3,480  of  5,350.    Elapsed: 0:18:01.\n",
      "  Batch 3,520  of  5,350.    Elapsed: 0:18:14.\n",
      "  Batch 3,560  of  5,350.    Elapsed: 0:18:26.\n",
      "  Batch 3,600  of  5,350.    Elapsed: 0:18:38.\n",
      "  Batch 3,640  of  5,350.    Elapsed: 0:18:52.\n",
      "  Batch 3,680  of  5,350.    Elapsed: 0:19:04.\n",
      "  Batch 3,720  of  5,350.    Elapsed: 0:19:17.\n",
      "  Batch 3,760  of  5,350.    Elapsed: 0:19:29.\n",
      "  Batch 3,800  of  5,350.    Elapsed: 0:19:42.\n",
      "  Batch 3,840  of  5,350.    Elapsed: 0:19:55.\n",
      "  Batch 3,880  of  5,350.    Elapsed: 0:20:07.\n",
      "  Batch 3,920  of  5,350.    Elapsed: 0:20:20.\n",
      "  Batch 3,960  of  5,350.    Elapsed: 0:20:32.\n",
      "  Batch 4,000  of  5,350.    Elapsed: 0:20:45.\n",
      "  Batch 4,040  of  5,350.    Elapsed: 0:20:57.\n",
      "  Batch 4,080  of  5,350.    Elapsed: 0:21:09.\n",
      "  Batch 4,120  of  5,350.    Elapsed: 0:21:21.\n",
      "  Batch 4,160  of  5,350.    Elapsed: 0:21:34.\n",
      "  Batch 4,200  of  5,350.    Elapsed: 0:21:46.\n",
      "  Batch 4,240  of  5,350.    Elapsed: 0:21:58.\n",
      "  Batch 4,280  of  5,350.    Elapsed: 0:22:11.\n",
      "  Batch 4,320  of  5,350.    Elapsed: 0:22:23.\n",
      "  Batch 4,360  of  5,350.    Elapsed: 0:22:37.\n",
      "  Batch 4,400  of  5,350.    Elapsed: 0:22:49.\n",
      "  Batch 4,440  of  5,350.    Elapsed: 0:23:01.\n",
      "  Batch 4,480  of  5,350.    Elapsed: 0:23:13.\n",
      "  Batch 4,520  of  5,350.    Elapsed: 0:23:26.\n",
      "  Batch 4,560  of  5,350.    Elapsed: 0:23:39.\n",
      "  Batch 4,600  of  5,350.    Elapsed: 0:23:51.\n",
      "  Batch 4,640  of  5,350.    Elapsed: 0:24:04.\n",
      "  Batch 4,680  of  5,350.    Elapsed: 0:24:16.\n",
      "  Batch 4,720  of  5,350.    Elapsed: 0:24:29.\n",
      "  Batch 4,760  of  5,350.    Elapsed: 0:24:42.\n",
      "  Batch 4,800  of  5,350.    Elapsed: 0:24:54.\n",
      "  Batch 4,840  of  5,350.    Elapsed: 0:25:07.\n",
      "  Batch 4,880  of  5,350.    Elapsed: 0:25:20.\n",
      "  Batch 4,920  of  5,350.    Elapsed: 0:25:32.\n",
      "  Batch 4,960  of  5,350.    Elapsed: 0:25:44.\n",
      "  Batch 5,000  of  5,350.    Elapsed: 0:25:57.\n",
      "  Batch 5,040  of  5,350.    Elapsed: 0:26:09.\n",
      "  Batch 5,080  of  5,350.    Elapsed: 0:26:21.\n",
      "  Batch 5,120  of  5,350.    Elapsed: 0:26:33.\n",
      "  Batch 5,160  of  5,350.    Elapsed: 0:26:46.\n",
      "  Batch 5,200  of  5,350.    Elapsed: 0:26:58.\n",
      "  Batch 5,240  of  5,350.    Elapsed: 0:27:10.\n",
      "  Batch 5,280  of  5,350.    Elapsed: 0:27:24.\n",
      "  Batch 5,320  of  5,350.    Elapsed: 0:27:36.\n",
      "\n",
      "  Average training loss: 1.30\n",
      "  Training epcoh took: 0:27:45\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.45\n",
      "  Validation took: 0:00:07\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  5,350.    Elapsed: 0:00:12.\n",
      "  Batch    80  of  5,350.    Elapsed: 0:00:25.\n",
      "  Batch   120  of  5,350.    Elapsed: 0:00:37.\n",
      "  Batch   160  of  5,350.    Elapsed: 0:00:49.\n",
      "  Batch   200  of  5,350.    Elapsed: 0:01:03.\n",
      "  Batch   240  of  5,350.    Elapsed: 0:01:15.\n",
      "  Batch   280  of  5,350.    Elapsed: 0:01:27.\n",
      "  Batch   320  of  5,350.    Elapsed: 0:01:40.\n",
      "  Batch   360  of  5,350.    Elapsed: 0:01:52.\n",
      "  Batch   400  of  5,350.    Elapsed: 0:02:05.\n",
      "  Batch   440  of  5,350.    Elapsed: 0:02:18.\n",
      "  Batch   480  of  5,350.    Elapsed: 0:02:30.\n",
      "  Batch   520  of  5,350.    Elapsed: 0:02:42.\n",
      "  Batch   560  of  5,350.    Elapsed: 0:02:56.\n",
      "  Batch   600  of  5,350.    Elapsed: 0:03:08.\n",
      "  Batch   640  of  5,350.    Elapsed: 0:03:21.\n",
      "  Batch   680  of  5,350.    Elapsed: 0:03:34.\n",
      "  Batch   720  of  5,350.    Elapsed: 0:03:46.\n",
      "  Batch   760  of  5,350.    Elapsed: 0:03:58.\n",
      "  Batch   800  of  5,350.    Elapsed: 0:04:11.\n",
      "  Batch   840  of  5,350.    Elapsed: 0:04:23.\n",
      "  Batch   880  of  5,350.    Elapsed: 0:04:35.\n",
      "  Batch   920  of  5,350.    Elapsed: 0:04:47.\n",
      "  Batch   960  of  5,350.    Elapsed: 0:05:00.\n",
      "  Batch 1,000  of  5,350.    Elapsed: 0:05:12.\n",
      "  Batch 1,040  of  5,350.    Elapsed: 0:05:24.\n",
      "  Batch 1,080  of  5,350.    Elapsed: 0:05:37.\n",
      "  Batch 1,120  of  5,350.    Elapsed: 0:05:50.\n",
      "  Batch 1,160  of  5,350.    Elapsed: 0:06:03.\n",
      "  Batch 1,200  of  5,350.    Elapsed: 0:06:15.\n",
      "  Batch 1,240  of  5,350.    Elapsed: 0:06:27.\n",
      "  Batch 1,280  of  5,350.    Elapsed: 0:06:40.\n",
      "  Batch 1,320  of  5,350.    Elapsed: 0:06:52.\n",
      "  Batch 1,360  of  5,350.    Elapsed: 0:07:06.\n",
      "  Batch 1,400  of  5,350.    Elapsed: 0:07:18.\n",
      "  Batch 1,440  of  5,350.    Elapsed: 0:07:30.\n",
      "  Batch 1,480  of  5,350.    Elapsed: 0:07:43.\n",
      "  Batch 1,520  of  5,350.    Elapsed: 0:07:56.\n",
      "  Batch 1,560  of  5,350.    Elapsed: 0:08:08.\n",
      "  Batch 1,600  of  5,350.    Elapsed: 0:08:21.\n",
      "  Batch 1,640  of  5,350.    Elapsed: 0:08:34.\n",
      "  Batch 1,680  of  5,350.    Elapsed: 0:08:46.\n",
      "  Batch 1,720  of  5,350.    Elapsed: 0:08:58.\n",
      "  Batch 1,760  of  5,350.    Elapsed: 0:09:10.\n",
      "  Batch 1,800  of  5,350.    Elapsed: 0:09:23.\n",
      "  Batch 1,840  of  5,350.    Elapsed: 0:09:35.\n",
      "  Batch 1,880  of  5,350.    Elapsed: 0:09:47.\n",
      "  Batch 1,920  of  5,350.    Elapsed: 0:09:59.\n",
      "  Batch 1,960  of  5,350.    Elapsed: 0:10:12.\n",
      "  Batch 2,000  of  5,350.    Elapsed: 0:10:24.\n",
      "  Batch 2,040  of  5,350.    Elapsed: 0:10:38.\n",
      "  Batch 2,080  of  5,350.    Elapsed: 0:10:50.\n",
      "  Batch 2,120  of  5,350.    Elapsed: 0:11:02.\n",
      "  Batch 2,160  of  5,350.    Elapsed: 0:11:15.\n",
      "  Batch 2,200  of  5,350.    Elapsed: 0:11:27.\n",
      "  Batch 2,240  of  5,350.    Elapsed: 0:11:39.\n",
      "  Batch 2,280  of  5,350.    Elapsed: 0:11:53.\n",
      "  Batch 2,320  of  5,350.    Elapsed: 0:12:05.\n",
      "  Batch 2,360  of  5,350.    Elapsed: 0:12:17.\n",
      "  Batch 2,400  of  5,350.    Elapsed: 0:12:30.\n",
      "  Batch 2,440  of  5,350.    Elapsed: 0:12:43.\n",
      "  Batch 2,480  of  5,350.    Elapsed: 0:12:56.\n",
      "  Batch 2,520  of  5,350.    Elapsed: 0:13:08.\n",
      "  Batch 2,560  of  5,350.    Elapsed: 0:13:21.\n",
      "  Batch 2,600  of  5,350.    Elapsed: 0:13:33.\n",
      "  Batch 2,640  of  5,350.    Elapsed: 0:13:45.\n",
      "  Batch 2,680  of  5,350.    Elapsed: 0:13:58.\n",
      "  Batch 2,720  of  5,350.    Elapsed: 0:14:10.\n",
      "  Batch 2,760  of  5,350.    Elapsed: 0:14:22.\n",
      "  Batch 2,800  of  5,350.    Elapsed: 0:14:34.\n",
      "  Batch 2,840  of  5,350.    Elapsed: 0:14:47.\n",
      "  Batch 2,880  of  5,350.    Elapsed: 0:14:59.\n",
      "  Batch 2,920  of  5,350.    Elapsed: 0:15:11.\n",
      "  Batch 2,960  of  5,350.    Elapsed: 0:15:24.\n",
      "  Batch 3,000  of  5,350.    Elapsed: 0:15:37.\n",
      "  Batch 3,040  of  5,350.    Elapsed: 0:15:50.\n",
      "  Batch 3,080  of  5,350.    Elapsed: 0:16:02.\n",
      "  Batch 3,120  of  5,350.    Elapsed: 0:16:14.\n",
      "  Batch 3,160  of  5,350.    Elapsed: 0:16:26.\n",
      "  Batch 3,200  of  5,350.    Elapsed: 0:16:40.\n",
      "  Batch 3,240  of  5,350.    Elapsed: 0:16:52.\n",
      "  Batch 3,280  of  5,350.    Elapsed: 0:17:05.\n",
      "  Batch 3,320  of  5,350.    Elapsed: 0:17:17.\n",
      "  Batch 3,360  of  5,350.    Elapsed: 0:17:30.\n",
      "  Batch 3,400  of  5,350.    Elapsed: 0:17:42.\n",
      "  Batch 3,440  of  5,350.    Elapsed: 0:17:54.\n",
      "  Batch 3,480  of  5,350.    Elapsed: 0:18:08.\n",
      "  Batch 3,520  of  5,350.    Elapsed: 0:18:20.\n",
      "  Batch 3,560  of  5,350.    Elapsed: 0:18:33.\n",
      "  Batch 3,600  of  5,350.    Elapsed: 0:18:45.\n",
      "  Batch 3,640  of  5,350.    Elapsed: 0:18:57.\n",
      "  Batch 3,680  of  5,350.    Elapsed: 0:19:10.\n",
      "  Batch 3,720  of  5,350.    Elapsed: 0:19:22.\n",
      "  Batch 3,760  of  5,350.    Elapsed: 0:19:34.\n",
      "  Batch 3,800  of  5,350.    Elapsed: 0:19:46.\n",
      "  Batch 3,840  of  5,350.    Elapsed: 0:19:59.\n",
      "  Batch 3,880  of  5,350.    Elapsed: 0:20:11.\n",
      "  Batch 3,920  of  5,350.    Elapsed: 0:20:25.\n",
      "  Batch 3,960  of  5,350.    Elapsed: 0:20:37.\n",
      "  Batch 4,000  of  5,350.    Elapsed: 0:20:49.\n",
      "  Batch 4,040  of  5,350.    Elapsed: 0:21:02.\n",
      "  Batch 4,080  of  5,350.    Elapsed: 0:21:14.\n",
      "  Batch 4,120  of  5,350.    Elapsed: 0:21:26.\n",
      "  Batch 4,160  of  5,350.    Elapsed: 0:21:40.\n",
      "  Batch 4,200  of  5,350.    Elapsed: 0:21:52.\n",
      "  Batch 4,240  of  5,350.    Elapsed: 0:22:04.\n",
      "  Batch 4,280  of  5,350.    Elapsed: 0:22:18.\n",
      "  Batch 4,320  of  5,350.    Elapsed: 0:22:30.\n",
      "  Batch 4,360  of  5,350.    Elapsed: 0:22:43.\n",
      "  Batch 4,400  of  5,350.    Elapsed: 0:22:56.\n",
      "  Batch 4,440  of  5,350.    Elapsed: 0:23:08.\n",
      "  Batch 4,480  of  5,350.    Elapsed: 0:23:20.\n",
      "  Batch 4,520  of  5,350.    Elapsed: 0:23:33.\n",
      "  Batch 4,560  of  5,350.    Elapsed: 0:23:45.\n",
      "  Batch 4,600  of  5,350.    Elapsed: 0:23:57.\n",
      "  Batch 4,640  of  5,350.    Elapsed: 0:24:09.\n",
      "  Batch 4,680  of  5,350.    Elapsed: 0:24:22.\n",
      "  Batch 4,720  of  5,350.    Elapsed: 0:24:34.\n",
      "  Batch 4,760  of  5,350.    Elapsed: 0:24:46.\n",
      "  Batch 4,800  of  5,350.    Elapsed: 0:24:59.\n",
      "  Batch 4,840  of  5,350.    Elapsed: 0:25:12.\n",
      "  Batch 4,880  of  5,350.    Elapsed: 0:25:24.\n",
      "  Batch 4,920  of  5,350.    Elapsed: 0:25:37.\n",
      "  Batch 4,960  of  5,350.    Elapsed: 0:25:49.\n",
      "  Batch 5,000  of  5,350.    Elapsed: 0:26:01.\n",
      "  Batch 5,040  of  5,350.    Elapsed: 0:26:14.\n",
      "  Batch 5,080  of  5,350.    Elapsed: 0:26:27.\n",
      "  Batch 5,120  of  5,350.    Elapsed: 0:26:40.\n",
      "  Batch 5,160  of  5,350.    Elapsed: 0:26:52.\n",
      "  Batch 5,200  of  5,350.    Elapsed: 0:27:04.\n",
      "  Batch 5,240  of  5,350.    Elapsed: 0:27:18.\n",
      "  Batch 5,280  of  5,350.    Elapsed: 0:27:30.\n",
      "  Batch 5,320  of  5,350.    Elapsed: 0:27:42.\n",
      "\n",
      "  Average training loss: 1.18\n",
      "  Training epcoh took: 0:27:52\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.51\n",
      "  Validation took: 0:00:07\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  5,350.    Elapsed: 0:00:12.\n",
      "  Batch    80  of  5,350.    Elapsed: 0:00:25.\n",
      "  Batch   120  of  5,350.    Elapsed: 0:00:38.\n",
      "  Batch   160  of  5,350.    Elapsed: 0:00:50.\n",
      "  Batch   200  of  5,350.    Elapsed: 0:01:03.\n",
      "  Batch   240  of  5,350.    Elapsed: 0:01:15.\n",
      "  Batch   280  of  5,350.    Elapsed: 0:01:27.\n",
      "  Batch   320  of  5,350.    Elapsed: 0:01:39.\n",
      "  Batch   360  of  5,350.    Elapsed: 0:01:52.\n",
      "  Batch   400  of  5,350.    Elapsed: 0:02:04.\n",
      "  Batch   440  of  5,350.    Elapsed: 0:02:16.\n",
      "  Batch   480  of  5,350.    Elapsed: 0:02:29.\n",
      "  Batch   520  of  5,350.    Elapsed: 0:02:41.\n",
      "  Batch   560  of  5,350.    Elapsed: 0:02:55.\n",
      "  Batch   600  of  5,350.    Elapsed: 0:03:07.\n",
      "  Batch   640  of  5,350.    Elapsed: 0:03:19.\n",
      "  Batch   680  of  5,350.    Elapsed: 0:03:32.\n",
      "  Batch   720  of  5,350.    Elapsed: 0:03:44.\n",
      "  Batch   760  of  5,350.    Elapsed: 0:03:58.\n",
      "  Batch   800  of  5,350.    Elapsed: 0:04:10.\n",
      "  Batch   840  of  5,350.    Elapsed: 0:04:22.\n",
      "  Batch   880  of  5,350.    Elapsed: 0:04:35.\n",
      "  Batch   920  of  5,350.    Elapsed: 0:04:48.\n",
      "  Batch   960  of  5,350.    Elapsed: 0:05:00.\n",
      "  Batch 1,000  of  5,350.    Elapsed: 0:05:13.\n",
      "  Batch 1,040  of  5,350.    Elapsed: 0:05:26.\n",
      "  Batch 1,080  of  5,350.    Elapsed: 0:05:38.\n",
      "  Batch 1,120  of  5,350.    Elapsed: 0:05:50.\n",
      "  Batch 1,160  of  5,350.    Elapsed: 0:06:03.\n",
      "  Batch 1,200  of  5,350.    Elapsed: 0:06:15.\n",
      "  Batch 1,240  of  5,350.    Elapsed: 0:06:27.\n",
      "  Batch 1,280  of  5,350.    Elapsed: 0:06:40.\n",
      "  Batch 1,320  of  5,350.    Elapsed: 0:06:52.\n",
      "  Batch 1,360  of  5,350.    Elapsed: 0:07:04.\n",
      "  Batch 1,400  of  5,350.    Elapsed: 0:07:17.\n",
      "  Batch 1,440  of  5,350.    Elapsed: 0:07:29.\n",
      "  Batch 1,480  of  5,350.    Elapsed: 0:07:43.\n",
      "  Batch 1,520  of  5,350.    Elapsed: 0:07:55.\n",
      "  Batch 1,560  of  5,350.    Elapsed: 0:08:07.\n",
      "  Batch 1,600  of  5,350.    Elapsed: 0:08:19.\n",
      "  Batch 1,640  of  5,350.    Elapsed: 0:08:32.\n",
      "  Batch 1,680  of  5,350.    Elapsed: 0:08:44.\n",
      "  Batch 1,720  of  5,350.    Elapsed: 0:08:58.\n",
      "  Batch 1,760  of  5,350.    Elapsed: 0:09:10.\n",
      "  Batch 1,800  of  5,350.    Elapsed: 0:09:22.\n",
      "  Batch 1,840  of  5,350.    Elapsed: 0:09:35.\n",
      "  Batch 1,880  of  5,350.    Elapsed: 0:09:48.\n",
      "  Batch 1,920  of  5,350.    Elapsed: 0:10:00.\n",
      "  Batch 1,960  of  5,350.    Elapsed: 0:10:12.\n",
      "  Batch 2,000  of  5,350.    Elapsed: 0:10:26.\n",
      "  Batch 2,040  of  5,350.    Elapsed: 0:10:38.\n",
      "  Batch 2,080  of  5,350.    Elapsed: 0:10:50.\n",
      "  Batch 2,120  of  5,350.    Elapsed: 0:11:03.\n",
      "  Batch 2,160  of  5,350.    Elapsed: 0:11:15.\n",
      "  Batch 2,200  of  5,350.    Elapsed: 0:11:27.\n",
      "  Batch 2,240  of  5,350.    Elapsed: 0:11:40.\n",
      "  Batch 2,280  of  5,350.    Elapsed: 0:11:52.\n",
      "  Batch 2,320  of  5,350.    Elapsed: 0:12:04.\n",
      "  Batch 2,360  of  5,350.    Elapsed: 0:12:16.\n",
      "  Batch 2,400  of  5,350.    Elapsed: 0:12:29.\n",
      "  Batch 2,440  of  5,350.    Elapsed: 0:12:42.\n",
      "  Batch 2,480  of  5,350.    Elapsed: 0:12:55.\n",
      "  Batch 2,520  of  5,350.    Elapsed: 0:13:07.\n",
      "  Batch 2,560  of  5,350.    Elapsed: 0:13:19.\n",
      "  Batch 2,600  of  5,350.    Elapsed: 0:13:32.\n",
      "  Batch 2,640  of  5,350.    Elapsed: 0:13:45.\n",
      "  Batch 2,680  of  5,350.    Elapsed: 0:13:57.\n",
      "  Batch 2,720  of  5,350.    Elapsed: 0:14:10.\n",
      "  Batch 2,760  of  5,350.    Elapsed: 0:14:22.\n",
      "  Batch 2,800  of  5,350.    Elapsed: 0:14:35.\n",
      "  Batch 2,840  of  5,350.    Elapsed: 0:14:48.\n",
      "  Batch 2,880  of  5,350.    Elapsed: 0:15:00.\n",
      "  Batch 2,920  of  5,350.    Elapsed: 0:15:13.\n",
      "  Batch 2,960  of  5,350.    Elapsed: 0:15:25.\n",
      "  Batch 3,000  of  5,350.    Elapsed: 0:15:38.\n",
      "  Batch 3,040  of  5,350.    Elapsed: 0:15:50.\n",
      "  Batch 3,080  of  5,350.    Elapsed: 0:16:02.\n",
      "  Batch 3,120  of  5,350.    Elapsed: 0:16:15.\n",
      "  Batch 3,160  of  5,350.    Elapsed: 0:16:27.\n",
      "  Batch 3,200  of  5,350.    Elapsed: 0:16:39.\n",
      "  Batch 3,240  of  5,350.    Elapsed: 0:16:52.\n",
      "  Batch 3,280  of  5,350.    Elapsed: 0:17:04.\n",
      "  Batch 3,320  of  5,350.    Elapsed: 0:17:16.\n",
      "  Batch 3,360  of  5,350.    Elapsed: 0:17:30.\n",
      "  Batch 3,400  of  5,350.    Elapsed: 0:17:42.\n",
      "  Batch 3,440  of  5,350.    Elapsed: 0:17:54.\n",
      "  Batch 3,480  of  5,350.    Elapsed: 0:18:06.\n",
      "  Batch 3,520  of  5,350.    Elapsed: 0:18:19.\n",
      "  Batch 3,560  of  5,350.    Elapsed: 0:18:31.\n",
      "  Batch 3,600  of  5,350.    Elapsed: 0:18:45.\n",
      "  Batch 3,640  of  5,350.    Elapsed: 0:18:57.\n",
      "  Batch 3,680  of  5,350.    Elapsed: 0:19:09.\n",
      "  Batch 3,720  of  5,350.    Elapsed: 0:19:23.\n",
      "  Batch 3,760  of  5,350.    Elapsed: 0:19:35.\n",
      "  Batch 3,800  of  5,350.    Elapsed: 0:19:48.\n",
      "  Batch 3,840  of  5,350.    Elapsed: 0:20:01.\n",
      "  Batch 3,880  of  5,350.    Elapsed: 0:20:13.\n",
      "  Batch 3,920  of  5,350.    Elapsed: 0:20:25.\n",
      "  Batch 3,960  of  5,350.    Elapsed: 0:20:38.\n",
      "  Batch 4,000  of  5,350.    Elapsed: 0:20:50.\n",
      "  Batch 4,040  of  5,350.    Elapsed: 0:21:02.\n",
      "  Batch 4,080  of  5,350.    Elapsed: 0:21:15.\n",
      "  Batch 4,120  of  5,350.    Elapsed: 0:21:27.\n",
      "  Batch 4,160  of  5,350.    Elapsed: 0:21:39.\n",
      "  Batch 4,200  of  5,350.    Elapsed: 0:21:51.\n",
      "  Batch 4,240  of  5,350.    Elapsed: 0:22:04.\n",
      "  Batch 4,280  of  5,350.    Elapsed: 0:22:17.\n",
      "  Batch 4,320  of  5,350.    Elapsed: 0:22:30.\n",
      "  Batch 4,360  of  5,350.    Elapsed: 0:22:42.\n",
      "  Batch 4,400  of  5,350.    Elapsed: 0:22:54.\n",
      "  Batch 4,440  of  5,350.    Elapsed: 0:23:07.\n",
      "  Batch 4,480  of  5,350.    Elapsed: 0:23:19.\n",
      "  Batch 4,520  of  5,350.    Elapsed: 0:23:32.\n",
      "  Batch 4,560  of  5,350.    Elapsed: 0:23:45.\n",
      "  Batch 4,600  of  5,350.    Elapsed: 0:23:57.\n",
      "  Batch 4,640  of  5,350.    Elapsed: 0:24:09.\n",
      "  Batch 4,680  of  5,350.    Elapsed: 0:24:23.\n",
      "  Batch 4,720  of  5,350.    Elapsed: 0:24:35.\n",
      "  Batch 4,760  of  5,350.    Elapsed: 0:24:47.\n",
      "  Batch 4,800  of  5,350.    Elapsed: 0:25:01.\n",
      "  Batch 4,840  of  5,350.    Elapsed: 0:25:13.\n",
      "  Batch 4,880  of  5,350.    Elapsed: 0:25:25.\n",
      "  Batch 4,920  of  5,350.    Elapsed: 0:25:38.\n",
      "  Batch 4,960  of  5,350.    Elapsed: 0:25:50.\n",
      "  Batch 5,000  of  5,350.    Elapsed: 0:26:02.\n",
      "  Batch 5,040  of  5,350.    Elapsed: 0:26:14.\n",
      "  Batch 5,080  of  5,350.    Elapsed: 0:26:27.\n",
      "  Batch 5,120  of  5,350.    Elapsed: 0:26:39.\n",
      "  Batch 5,160  of  5,350.    Elapsed: 0:26:51.\n",
      "  Batch 5,200  of  5,350.    Elapsed: 0:27:04.\n",
      "  Batch 5,240  of  5,350.    Elapsed: 0:27:17.\n",
      "  Batch 5,280  of  5,350.    Elapsed: 0:27:30.\n",
      "  Batch 5,320  of  5,350.    Elapsed: 0:27:42.\n",
      "\n",
      "  Average training loss: 0.98\n",
      "  Training epcoh took: 0:27:51\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.49\n",
      "  Validation took: 0:00:07\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  5,350.    Elapsed: 0:00:12.\n",
      "  Batch    80  of  5,350.    Elapsed: 0:00:26.\n",
      "  Batch   120  of  5,350.    Elapsed: 0:00:38.\n",
      "  Batch   160  of  5,350.    Elapsed: 0:00:50.\n",
      "  Batch   200  of  5,350.    Elapsed: 0:01:03.\n",
      "  Batch   240  of  5,350.    Elapsed: 0:01:15.\n",
      "  Batch   280  of  5,350.    Elapsed: 0:01:29.\n",
      "  Batch   320  of  5,350.    Elapsed: 0:01:41.\n",
      "  Batch   360  of  5,350.    Elapsed: 0:01:53.\n",
      "  Batch   400  of  5,350.    Elapsed: 0:02:05.\n",
      "  Batch   440  of  5,350.    Elapsed: 0:02:19.\n",
      "  Batch   480  of  5,350.    Elapsed: 0:02:31.\n",
      "  Batch   520  of  5,350.    Elapsed: 0:02:44.\n",
      "  Batch   560  of  5,350.    Elapsed: 0:02:57.\n",
      "  Batch   600  of  5,350.    Elapsed: 0:03:09.\n",
      "  Batch   640  of  5,350.    Elapsed: 0:03:21.\n",
      "  Batch   680  of  5,350.    Elapsed: 0:03:34.\n",
      "  Batch   720  of  5,350.    Elapsed: 0:03:46.\n",
      "  Batch   760  of  5,350.    Elapsed: 0:03:58.\n",
      "  Batch   800  of  5,350.    Elapsed: 0:04:11.\n",
      "  Batch   840  of  5,350.    Elapsed: 0:04:23.\n",
      "  Batch   880  of  5,350.    Elapsed: 0:04:35.\n",
      "  Batch   920  of  5,350.    Elapsed: 0:04:48.\n",
      "  Batch   960  of  5,350.    Elapsed: 0:05:00.\n",
      "  Batch 1,000  of  5,350.    Elapsed: 0:05:14.\n",
      "  Batch 1,040  of  5,350.    Elapsed: 0:05:26.\n",
      "  Batch 1,080  of  5,350.    Elapsed: 0:05:38.\n",
      "  Batch 1,120  of  5,350.    Elapsed: 0:05:51.\n",
      "  Batch 1,160  of  5,350.    Elapsed: 0:06:03.\n",
      "  Batch 1,200  of  5,350.    Elapsed: 0:06:15.\n",
      "  Batch 1,240  of  5,350.    Elapsed: 0:06:29.\n",
      "  Batch 1,280  of  5,350.    Elapsed: 0:06:41.\n",
      "  Batch 1,320  of  5,350.    Elapsed: 0:06:53.\n",
      "  Batch 1,360  of  5,350.    Elapsed: 0:07:06.\n",
      "  Batch 1,400  of  5,350.    Elapsed: 0:07:19.\n",
      "  Batch 1,440  of  5,350.    Elapsed: 0:07:32.\n",
      "  Batch 1,480  of  5,350.    Elapsed: 0:07:44.\n",
      "  Batch 1,520  of  5,350.    Elapsed: 0:07:57.\n",
      "  Batch 1,560  of  5,350.    Elapsed: 0:08:09.\n",
      "  Batch 1,600  of  5,350.    Elapsed: 0:08:22.\n",
      "  Batch 1,640  of  5,350.    Elapsed: 0:08:34.\n",
      "  Batch 1,680  of  5,350.    Elapsed: 0:08:46.\n",
      "  Batch 1,720  of  5,350.    Elapsed: 0:08:58.\n",
      "  Batch 1,760  of  5,350.    Elapsed: 0:09:11.\n",
      "  Batch 1,800  of  5,350.    Elapsed: 0:09:23.\n",
      "  Batch 1,840  of  5,350.    Elapsed: 0:09:35.\n",
      "  Batch 1,880  of  5,350.    Elapsed: 0:09:47.\n",
      "  Batch 1,920  of  5,350.    Elapsed: 0:10:01.\n",
      "  Batch 1,960  of  5,350.    Elapsed: 0:10:13.\n",
      "  Batch 2,000  of  5,350.    Elapsed: 0:10:26.\n",
      "  Batch 2,040  of  5,350.    Elapsed: 0:10:38.\n",
      "  Batch 2,080  of  5,350.    Elapsed: 0:10:50.\n",
      "  Batch 2,120  of  5,350.    Elapsed: 0:11:03.\n",
      "  Batch 2,160  of  5,350.    Elapsed: 0:11:16.\n",
      "  Batch 2,200  of  5,350.    Elapsed: 0:11:28.\n",
      "  Batch 2,240  of  5,350.    Elapsed: 0:11:41.\n",
      "  Batch 2,280  of  5,350.    Elapsed: 0:11:53.\n",
      "  Batch 2,320  of  5,350.    Elapsed: 0:12:06.\n",
      "  Batch 2,360  of  5,350.    Elapsed: 0:12:19.\n",
      "  Batch 2,400  of  5,350.    Elapsed: 0:12:31.\n",
      "  Batch 2,440  of  5,350.    Elapsed: 0:12:44.\n",
      "  Batch 2,480  of  5,350.    Elapsed: 0:12:57.\n",
      "  Batch 2,520  of  5,350.    Elapsed: 0:13:09.\n",
      "  Batch 2,560  of  5,350.    Elapsed: 0:13:21.\n",
      "  Batch 2,600  of  5,350.    Elapsed: 0:13:34.\n",
      "  Batch 2,640  of  5,350.    Elapsed: 0:13:46.\n",
      "  Batch 2,680  of  5,350.    Elapsed: 0:13:58.\n",
      "  Batch 2,720  of  5,350.    Elapsed: 0:14:10.\n",
      "  Batch 2,760  of  5,350.    Elapsed: 0:14:23.\n",
      "  Batch 2,800  of  5,350.    Elapsed: 0:14:35.\n",
      "  Batch 2,840  of  5,350.    Elapsed: 0:14:47.\n",
      "  Batch 2,880  of  5,350.    Elapsed: 0:15:01.\n",
      "  Batch 2,920  of  5,350.    Elapsed: 0:15:13.\n",
      "  Batch 2,960  of  5,350.    Elapsed: 0:15:26.\n",
      "  Batch 3,000  of  5,350.    Elapsed: 0:15:38.\n",
      "  Batch 3,040  of  5,350.    Elapsed: 0:15:50.\n",
      "  Batch 3,080  of  5,350.    Elapsed: 0:16:04.\n",
      "  Batch 3,120  of  5,350.    Elapsed: 0:16:16.\n",
      "  Batch 3,160  of  5,350.    Elapsed: 0:16:28.\n",
      "  Batch 3,200  of  5,350.    Elapsed: 0:16:41.\n",
      "  Batch 3,240  of  5,350.    Elapsed: 0:16:54.\n",
      "  Batch 3,280  of  5,350.    Elapsed: 0:17:07.\n",
      "  Batch 3,320  of  5,350.    Elapsed: 0:17:19.\n",
      "  Batch 3,360  of  5,350.    Elapsed: 0:17:32.\n",
      "  Batch 3,400  of  5,350.    Elapsed: 0:17:44.\n",
      "  Batch 3,440  of  5,350.    Elapsed: 0:17:57.\n",
      "  Batch 3,480  of  5,350.    Elapsed: 0:18:09.\n",
      "  Batch 3,520  of  5,350.    Elapsed: 0:18:21.\n",
      "  Batch 3,560  of  5,350.    Elapsed: 0:18:33.\n",
      "  Batch 3,600  of  5,350.    Elapsed: 0:18:46.\n",
      "  Batch 3,640  of  5,350.    Elapsed: 0:18:58.\n",
      "  Batch 3,680  of  5,350.    Elapsed: 0:19:10.\n",
      "  Batch 3,720  of  5,350.    Elapsed: 0:19:23.\n",
      "  Batch 3,760  of  5,350.    Elapsed: 0:19:35.\n",
      "  Batch 3,800  of  5,350.    Elapsed: 0:19:49.\n",
      "  Batch 3,840  of  5,350.    Elapsed: 0:20:01.\n",
      "  Batch 3,880  of  5,350.    Elapsed: 0:20:13.\n",
      "  Batch 3,920  of  5,350.    Elapsed: 0:20:26.\n",
      "  Batch 3,960  of  5,350.    Elapsed: 0:20:38.\n",
      "  Batch 4,000  of  5,350.    Elapsed: 0:20:50.\n",
      "  Batch 4,040  of  5,350.    Elapsed: 0:21:04.\n",
      "  Batch 4,080  of  5,350.    Elapsed: 0:21:16.\n",
      "  Batch 4,120  of  5,350.    Elapsed: 0:21:28.\n",
      "  Batch 4,160  of  5,350.    Elapsed: 0:21:41.\n",
      "  Batch 4,200  of  5,350.    Elapsed: 0:21:54.\n",
      "  Batch 4,240  of  5,350.    Elapsed: 0:22:06.\n",
      "  Batch 4,280  of  5,350.    Elapsed: 0:22:18.\n",
      "  Batch 4,320  of  5,350.    Elapsed: 0:22:32.\n",
      "  Batch 4,360  of  5,350.    Elapsed: 0:22:44.\n",
      "  Batch 4,400  of  5,350.    Elapsed: 0:22:56.\n",
      "  Batch 4,440  of  5,350.    Elapsed: 0:23:09.\n",
      "  Batch 4,480  of  5,350.    Elapsed: 0:23:21.\n",
      "  Batch 4,520  of  5,350.    Elapsed: 0:23:33.\n",
      "  Batch 4,560  of  5,350.    Elapsed: 0:23:46.\n",
      "  Batch 4,600  of  5,350.    Elapsed: 0:23:58.\n",
      "  Batch 4,640  of  5,350.    Elapsed: 0:24:10.\n",
      "  Batch 4,680  of  5,350.    Elapsed: 0:24:23.\n",
      "  Batch 4,720  of  5,350.    Elapsed: 0:24:35.\n",
      "  Batch 4,760  of  5,350.    Elapsed: 0:24:49.\n",
      "  Batch 4,800  of  5,350.    Elapsed: 0:25:01.\n",
      "  Batch 4,840  of  5,350.    Elapsed: 0:25:13.\n",
      "  Batch 4,880  of  5,350.    Elapsed: 0:25:25.\n",
      "  Batch 4,920  of  5,350.    Elapsed: 0:25:38.\n",
      "  Batch 4,960  of  5,350.    Elapsed: 0:25:51.\n",
      "  Batch 5,000  of  5,350.    Elapsed: 0:26:04.\n",
      "  Batch 5,040  of  5,350.    Elapsed: 0:26:16.\n",
      "  Batch 5,080  of  5,350.    Elapsed: 0:26:28.\n",
      "  Batch 5,120  of  5,350.    Elapsed: 0:26:41.\n",
      "  Batch 5,160  of  5,350.    Elapsed: 0:26:54.\n",
      "  Batch 5,200  of  5,350.    Elapsed: 0:27:06.\n",
      "  Batch 5,240  of  5,350.    Elapsed: 0:27:20.\n",
      "  Batch 5,280  of  5,350.    Elapsed: 0:27:32.\n",
      "  Batch 5,320  of  5,350.    Elapsed: 0:27:44.\n",
      "\n",
      "  Average training loss: 0.84\n",
      "  Training epcoh took: 0:27:53\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.48\n",
      "  Validation took: 0:00:07\n",
      "\n",
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/Partha/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZPeyGKYtoCn",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EFogz7jeiycc",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Helper function for predicting whether label matches the person's prediction, for one person:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "def predict_person(text, label, length):\n",
    "  #  print(len(text))\n",
    "    textLen = len(text)\n",
    "    sentences = [text[0 + i * int(textLen/30) : (i + 1) * int(textLen/30)] for i in range(0, 30)]\n",
    "  #  print(len(sentences))\n",
    "    labels = [label] * len(sentences)\n",
    "  #  print(len(labels))\n",
    "   # print(len(sentences))\n",
    "    input_ids = []\n",
    "    for sent in sentences:\n",
    "    # `encode` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "        encoded_sent = tokenizer.encode(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                   )\n",
    "\n",
    "        input_ids.append(encoded_sent)\n",
    "\n",
    "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN,\n",
    "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "    # Create attention masks\n",
    "    attention_masks = []\n",
    "\n",
    "    # Create a mask of 1s for each token followed by 0s for padding\n",
    "    for seq in input_ids:\n",
    "        seq_mask = [float(i>0) for i in seq]\n",
    "        attention_masks.append(seq_mask)\n",
    "\n",
    "    # Convert to tensors.\n",
    "    prediction_inputs = torch.tensor(input_ids)\n",
    "    prediction_masks = torch.tensor(attention_masks)\n",
    "    prediction_labels = torch.tensor(labels)\n",
    "    # Set the batch size.\n",
    "    batch_size = 32\n",
    "\n",
    "    # Create the DataLoader.\n",
    "    prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
    "    prediction_sampler = SequentialSampler(prediction_data)\n",
    "    prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
    "\n",
    "  #  print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
    "\n",
    "    # Put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    predictions , true_labels = [], []\n",
    "\n",
    "    # Predict\n",
    "    for batch in prediction_dataloader:\n",
    "    # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    # Telling the model not to compute or store gradients, saving memory and\n",
    "    # speeding up prediction\n",
    "        with torch.no_grad():\n",
    "           # Forward pass, calculate logit predictions\n",
    "           outputs = model(b_input_ids, token_type_ids=None,\n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "        logits = outputs[0]\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Store predictions and true labels\n",
    "        predictions.append(logits)\n",
    "        true_labels.append(label_ids)\n",
    "\n",
    "    #for i in range(len(true_labels)):\n",
    "\n",
    "    # The predictions for this batch are a 2-column ndarray (one column for \"0\"\n",
    "    # and one column for \"1\"). Pick the label with the highest value and turn this\n",
    "    # in to a list of 0s and 1s.\n",
    "    correct = 0\n",
    "    num = 0\n",
    "\n",
    "    final_predict_list = []\n",
    "\n",
    "    for i in range(len(true_labels)):\n",
    "\n",
    "        pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
    "        [final_predict_list.append(i) for i in pred_labels_i]\n",
    "       # if accuracy_score(pred_labels_i, [0] * len(pred_labels_i)) < 0.5 :\n",
    "        #    correct += 1\n",
    "       # num+=1\n",
    "  #  print(final_predict_list)\n",
    "\n",
    "    if (statistics.mode(final_predict_list)==label):\n",
    "        return 1\n",
    "\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "data": {
      "text/plain": "                liked_by                                               text  \\\n0     873952654392647681  Champ and Major have joined us in the White Ho...   \n1               32089349  We did this—together.I hope Mike Pence smiles ...   \n2               36685290  Little kids are obsessed with garbage trucks b...   \n3                7885362  Don’t worry, Mr. President. I’ll see you at yo...   \n4    1065701994512424960  Happy Pancake day NOW fans! Best toppings go.....   \n..                   ...                                                ...   \n279  1142510423285145600  Took her phone and texted her gc “guess who I’...   \n280  1039946712972976129  @VoyageoftheMind @VoyageoftheMind Yes definite...   \n281           1155187406  NFL Black Lives Matter commercial showing play...   \n282             89410598  @BOHE_BABE @hongokucho \\nhttps://t.co/a2xd7R3h...   \n283  1054197127608442880  I want whatever the people who run at 6am have...   \n\n     type  extravert  intuitive  thinking  judging  NT  SF  NF  ST  NJ  NP  \\\n0    ENFJ          1          1         0        1   0   0   1   0   1   0   \n1    ENFJ          1          1         0        1   0   0   1   0   1   0   \n2    INTJ          0          1         1        1   1   0   0   0   1   0   \n3    INTJ          0          1         1        1   1   0   0   0   1   0   \n4    ISFP          0          0         0        0   0   1   0   0   0   0   \n..    ...        ...        ...       ...      ...  ..  ..  ..  ..  ..  ..   \n279  ENFP          1          1         0        0   0   0   1   0   0   1   \n280  ENTP          1          1         1        0   1   0   0   0   0   1   \n281  INFJ          0          1         0        1   0   0   1   0   1   0   \n282  ENTP          1          1         1        0   1   0   0   0   0   1   \n283  ISFP          0          0         0        0   0   1   0   0   0   0   \n\n     SJ  SP  role  \n0     0   0     1  \n1     0   0     1  \n2     0   0     2  \n3     0   0     2  \n4     0   1     0  \n..   ..  ..   ...  \n279   0   0     1  \n280   0   0     2  \n281   0   0     1  \n282   0   0     2  \n283   0   1     0  \n\n[284 rows x 16 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>liked_by</th>\n      <th>text</th>\n      <th>type</th>\n      <th>extravert</th>\n      <th>intuitive</th>\n      <th>thinking</th>\n      <th>judging</th>\n      <th>NT</th>\n      <th>SF</th>\n      <th>NF</th>\n      <th>ST</th>\n      <th>NJ</th>\n      <th>NP</th>\n      <th>SJ</th>\n      <th>SP</th>\n      <th>role</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>873952654392647681</td>\n      <td>Champ and Major have joined us in the White Ho...</td>\n      <td>ENFJ</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>32089349</td>\n      <td>We did this—together.I hope Mike Pence smiles ...</td>\n      <td>ENFJ</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>36685290</td>\n      <td>Little kids are obsessed with garbage trucks b...</td>\n      <td>INTJ</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7885362</td>\n      <td>Don’t worry, Mr. President. I’ll see you at yo...</td>\n      <td>INTJ</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1065701994512424960</td>\n      <td>Happy Pancake day NOW fans! Best toppings go.....</td>\n      <td>ISFP</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>279</th>\n      <td>1142510423285145600</td>\n      <td>Took her phone and texted her gc “guess who I’...</td>\n      <td>ENFP</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>280</th>\n      <td>1039946712972976129</td>\n      <td>@VoyageoftheMind @VoyageoftheMind Yes definite...</td>\n      <td>ENTP</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>281</th>\n      <td>1155187406</td>\n      <td>NFL Black Lives Matter commercial showing play...</td>\n      <td>INFJ</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>282</th>\n      <td>89410598</td>\n      <td>@BOHE_BABE @hongokucho \\nhttps://t.co/a2xd7R3h...</td>\n      <td>ENTP</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>283</th>\n      <td>1054197127608442880</td>\n      <td>I want whatever the people who run at 6am have...</td>\n      <td>ISFP</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>284 rows × 16 columns</p>\n</div>"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_df = test_df[test_df['role']==1].sample(71)\n",
    "testing_df = testing_df.append(test_df[test_df['role']==2].sample(71))\n",
    "testing_df = testing_df.append(test_df[test_df['role']==3].sample(71))\n",
    "testing_df = testing_df.append(test_df[test_df['role']==0].sample(71))\n",
    "\n",
    "testing_df = testing_df.sample(frac = 1, random_state= 543)\n",
    "testing_df = testing_df.reset_index(drop = True)\n",
    "\n",
    "testing_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Final predictive accuracy score:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testing..:   0%|          | 1/284 [00:00<01:57,  2.40it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (539 > 512). Running this sequence through the model will result in indexing errors\n",
      "testing..: 100%|██████████| 284/284 [01:41<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:  0.4119718309859155\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "num = 0\n",
    "for row in tqdm(range(0,len(testing_df.index)), \"testing..\"):\n",
    "    num += 1\n",
    "    pred_text = testing_df.at[row, 'text']\n",
    "    pred_label = testing_df.at[row, 'role']\n",
    "    pred = predict_person(pred_text, pred_label, 256)\n",
    "   # print(pred)\n",
    "    if pred == 1:\n",
    "        correct+=1\n",
    "\n",
    "print(\"ACC: \", correct/num)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Saving model to disk"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to BERT_roles\n"
     ]
    },
    {
     "data": {
      "text/plain": "('BERT_roles/tokenizer_config.json',\n 'BERT_roles/special_tokens_map.json',\n 'BERT_roles/vocab.txt',\n 'BERT_roles/added_tokens.json')"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "\n",
    "output_dir = 'BERT_roles'\n",
    "\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "# They can then be reloaded using `from_pretrained()`\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Good practice: save your training arguments together with the trained model\n",
    "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Sandbox (the outputs from these code chunks are not up to date and were performed ad hoc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [],
   "source": [
    "raw_df = raw_df.drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "raw_df\n",
    "raw_set = set(raw_df['liked_by'])\n",
    "print(len(raw_set))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 128,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4186\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(raw_df, stratify = raw_df['type'], random_state= 1729, test_size= 0.12)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3680\n"
     ]
    }
   ],
   "source": [
    "train_set = set(edf['liked_by'])\n",
    "print(len(train_set))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "503\n"
     ]
    }
   ],
   "source": [
    "test_set = set(test_df['liked_by'])\n",
    "print(len(test_set))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for val in set(test_df['liked_by']):\n",
    "    if val in train_set:\n",
    "        i+=1\n",
    "        print(val)\n",
    "        print('dude wtf')\n",
    "print(i)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [
    {
     "data": {
      "text/plain": "                 liked_by                                               text  \\\n1827   856944637575090176  doing this for Rue. I couldn’t type all I’m fe...   \n3396   856944637575090176   a medal of freedom tbh... your move Biden@sar...   \n4854   856944637575090176  rl who raises the dead for $$$. Out 8.24.2021!...   \n13563  856944637575090176   trying to tell someone I didn’t believe in me...   \n18242  856944637575090176  is FIVE DAYS AWAY!!! Since it’s almost the new...   \n19429  856944637575090176  ir wine with food\\n\\nA dark god with fine tast...   \n32099  856944637575090176  solid Norse number, I commissioned 9 character...   \n32897  856944637575090176  \\n\\nwhat the hell did they expect me to eat th...   \n33639  856944637575090176  s part out during revisions, what if the chapt...   \n34696  856944637575090176  /t.co/ArcG41MLtG@sarahcatstreet b4$ !!Just got...   \n36578  856944637575090176   the same notes over and over, so here is Quer...   \n44558  856944637575090176  o icon-ify me, and i'm obsessed with the resul...   \n46841  856944637575090176  ised for a yearwhen you’ve been away from your...   \n51946  856944637575090176  to my twenties: you brought me all the best an...   \n56783  856944637575090176   were right, it CAN kill a human being in unde...   \n61720  856944637575090176  only to teach you what absolutely doesn’t work...   \n63004  856944637575090176   jace herondale.\\n\\nthat’s it. that’s the twee...   \n63251  856944637575090176  quel to #lotr. Set in the era of Aragorn's son...   \n64260  856944637575090176  will show and six of you will be dead.” https:...   \n69200  856944637575090176  utine. https://t.co/ZPGjLGmdhS@sarahcatstreet ...   \n72638  856944637575090176  s the same name as one of my children #ammteas...   \n73872  856944637575090176  o.  \\n\\n(And, hey, that's longer than my last ...   \n75666  856944637575090176  ink you all probably know this already but jus...   \n77253  856944637575090176  . https://t.co/XExDGWc32xnever stan real peopl...   \n77453  856944637575090176   to 2016 tumblr ever againWant blue check unde...   \n79255  856944637575090176  gothsI saw BWB’s ARC for the first time. I ope...   \n84801  856944637575090176  .... no further notes on this for now just pro...   \n90797  856944637575090176  ’s puppy had to have emergency surgery and the...   \n91206  856944637575090176  r holy shitmight do a newsletter this week wit...   \n\n       type  extravert  intuitive  thinking  judging  NT  SF  NF  ST  NJ  NP  \\\n1827   ENTJ          1          1         1        1   1   0   0   0   1   0   \n3396   ENTJ          1          1         1        1   1   0   0   0   1   0   \n4854   ENTJ          1          1         1        1   1   0   0   0   1   0   \n13563  ENTJ          1          1         1        1   1   0   0   0   1   0   \n18242  ENTJ          1          1         1        1   1   0   0   0   1   0   \n19429  ENTJ          1          1         1        1   1   0   0   0   1   0   \n32099  ENTJ          1          1         1        1   1   0   0   0   1   0   \n32897  ENTJ          1          1         1        1   1   0   0   0   1   0   \n33639  ENTJ          1          1         1        1   1   0   0   0   1   0   \n34696  ENTJ          1          1         1        1   1   0   0   0   1   0   \n36578  ENTJ          1          1         1        1   1   0   0   0   1   0   \n44558  ENTJ          1          1         1        1   1   0   0   0   1   0   \n46841  ENTJ          1          1         1        1   1   0   0   0   1   0   \n51946  ENTJ          1          1         1        1   1   0   0   0   1   0   \n56783  ENTJ          1          1         1        1   1   0   0   0   1   0   \n61720  ENTJ          1          1         1        1   1   0   0   0   1   0   \n63004  ENTJ          1          1         1        1   1   0   0   0   1   0   \n63251  ENTJ          1          1         1        1   1   0   0   0   1   0   \n64260  ENTJ          1          1         1        1   1   0   0   0   1   0   \n69200  ENTJ          1          1         1        1   1   0   0   0   1   0   \n72638  ENTJ          1          1         1        1   1   0   0   0   1   0   \n73872  ENTJ          1          1         1        1   1   0   0   0   1   0   \n75666  ENTJ          1          1         1        1   1   0   0   0   1   0   \n77253  ENTJ          1          1         1        1   1   0   0   0   1   0   \n77453  ENTJ          1          1         1        1   1   0   0   0   1   0   \n79255  ENTJ          1          1         1        1   1   0   0   0   1   0   \n84801  ENTJ          1          1         1        1   1   0   0   0   1   0   \n90797  ENTJ          1          1         1        1   1   0   0   0   1   0   \n91206  ENTJ          1          1         1        1   1   0   0   0   1   0   \n\n       SJ  SP                                    tokenized_texts  \n1827    0   0  ['[CLS]', 'doing', 'this', 'for', 'rue', '.', ...  \n3396    0   0  ['[CLS]', 'a', 'medal', 'of', 'freedom', 'tb',...  \n4854    0   0  ['[CLS]', 'r', '##l', 'who', 'raises', 'the', ...  \n13563   0   0  ['[CLS]', 'trying', 'to', 'tell', 'someone', '...  \n18242   0   0  ['[CLS]', 'is', 'five', 'days', 'away', '!', '...  \n19429   0   0  ['[CLS]', 'ir', 'wine', 'with', 'food', 'a', '...  \n32099   0   0  ['[CLS]', 'solid', 'norse', 'number', ',', 'i'...  \n32897   0   0  ['[CLS]', 'what', 'the', 'hell', 'did', 'they'...  \n33639   0   0  ['[CLS]', 's', 'part', 'out', 'during', 'revis...  \n34696   0   0  ['[CLS]', '/', 't', '.', 'co', '/', 'arc', '##...  \n36578   0   0  ['[CLS]', 'the', 'same', 'notes', 'over', 'and...  \n44558   0   0  ['[CLS]', 'o', 'icon', '-', 'if', '##y', 'me',...  \n46841   0   0  ['[CLS]', 'is', '##ed', 'for', 'a', 'year', '#...  \n51946   0   0  ['[CLS]', 'to', 'my', 'twenties', ':', 'you', ...  \n56783   0   0  ['[CLS]', 'were', 'right', ',', 'it', 'can', '...  \n61720   0   0  ['[CLS]', 'only', 'to', 'teach', 'you', 'what'...  \n63004   0   0  ['[CLS]', 'jace', 'heron', '##dale', '.', 'tha...  \n63251   0   0  ['[CLS]', 'que', '##l', 'to', '#', 'lot', '##r...  \n64260   0   0  ['[CLS]', 'will', 'show', 'and', 'six', 'of', ...  \n69200   0   0  ['[CLS]', 'ut', '##ine', '.', 'https', ':', '/...  \n72638   0   0  ['[CLS]', 's', 'the', 'same', 'name', 'as', 'o...  \n73872   0   0  ['[CLS]', 'o', '.', '(', 'and', ',', 'hey', ',...  \n75666   0   0  ['[CLS]', 'ink', 'you', 'all', 'probably', 'kn...  \n77253   0   0  ['[CLS]', '.', 'https', ':', '/', '/', 't', '....  \n77453   0   0  ['[CLS]', 'to', '2016', 'tu', '##mb', '##lr', ...  \n79255   0   0  ['[CLS]', 'goth', '##si', 'saw', 'b', '##w', '...  \n84801   0   0  ['[CLS]', '.', '.', '.', '.', 'no', 'further',...  \n90797   0   0  ['[CLS]', '’', 's', 'puppy', 'had', 'to', 'hav...  \n91206   0   0  ['[CLS]', 'r', 'holy', 'shit', '##mi', '##ght'...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>liked_by</th>\n      <th>text</th>\n      <th>type</th>\n      <th>extravert</th>\n      <th>intuitive</th>\n      <th>thinking</th>\n      <th>judging</th>\n      <th>NT</th>\n      <th>SF</th>\n      <th>NF</th>\n      <th>ST</th>\n      <th>NJ</th>\n      <th>NP</th>\n      <th>SJ</th>\n      <th>SP</th>\n      <th>tokenized_texts</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1827</th>\n      <td>856944637575090176</td>\n      <td>doing this for Rue. I couldn’t type all I’m fe...</td>\n      <td>ENTJ</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['[CLS]', 'doing', 'this', 'for', 'rue', '.', ...</td>\n    </tr>\n    <tr>\n      <th>3396</th>\n      <td>856944637575090176</td>\n      <td>a medal of freedom tbh... your move Biden@sar...</td>\n      <td>ENTJ</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['[CLS]', 'a', 'medal', 'of', 'freedom', 'tb',...</td>\n    </tr>\n    <tr>\n      <th>4854</th>\n      <td>856944637575090176</td>\n      <td>rl who raises the dead for $$$. Out 8.24.2021!...</td>\n      <td>ENTJ</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['[CLS]', 'r', '##l', 'who', 'raises', 'the', ...</td>\n    </tr>\n    <tr>\n      <th>13563</th>\n      <td>856944637575090176</td>\n      <td>trying to tell someone I didn’t believe in me...</td>\n      <td>ENTJ</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['[CLS]', 'trying', 'to', 'tell', 'someone', '...</td>\n    </tr>\n    <tr>\n      <th>18242</th>\n      <td>856944637575090176</td>\n      <td>is FIVE DAYS AWAY!!! Since it’s almost the new...</td>\n      <td>ENTJ</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['[CLS]', 'is', 'five', 'days', 'away', '!', '...</td>\n    </tr>\n    <tr>\n      <th>19429</th>\n      <td>856944637575090176</td>\n      <td>ir wine with food\\n\\nA dark god with fine tast...</td>\n      <td>ENTJ</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['[CLS]', 'ir', 'wine', 'with', 'food', 'a', '...</td>\n    </tr>\n    <tr>\n      <th>32099</th>\n      <td>856944637575090176</td>\n      <td>solid Norse number, I commissioned 9 character...</td>\n      <td>ENTJ</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['[CLS]', 'solid', 'norse', 'number', ',', 'i'...</td>\n    </tr>\n    <tr>\n      <th>32897</th>\n      <td>856944637575090176</td>\n      <td>\\n\\nwhat the hell did they expect me to eat th...</td>\n      <td>ENTJ</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['[CLS]', 'what', 'the', 'hell', 'did', 'they'...</td>\n    </tr>\n    <tr>\n      <th>33639</th>\n      <td>856944637575090176</td>\n      <td>s part out during revisions, what if the chapt...</td>\n      <td>ENTJ</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['[CLS]', 's', 'part', 'out', 'during', 'revis...</td>\n    </tr>\n    <tr>\n      <th>34696</th>\n      <td>856944637575090176</td>\n      <td>/t.co/ArcG41MLtG@sarahcatstreet b4$ !!Just got...</td>\n      <td>ENTJ</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['[CLS]', '/', 't', '.', 'co', '/', 'arc', '##...</td>\n    </tr>\n    <tr>\n      <th>36578</th>\n      <td>856944637575090176</td>\n      <td>the same notes over and over, so here is Quer...</td>\n      <td>ENTJ</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['[CLS]', 'the', 'same', 'notes', 'over', 'and...</td>\n    </tr>\n    <tr>\n      <th>44558</th>\n      <td>856944637575090176</td>\n      <td>o icon-ify me, and i'm obsessed with the resul...</td>\n      <td>ENTJ</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['[CLS]', 'o', 'icon', '-', 'if', '##y', 'me',...</td>\n    </tr>\n    <tr>\n      <th>46841</th>\n      <td>856944637575090176</td>\n      <td>ised for a yearwhen you’ve been away from your...</td>\n      <td>ENTJ</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['[CLS]', 'is', '##ed', 'for', 'a', 'year', '#...</td>\n    </tr>\n    <tr>\n      <th>51946</th>\n      <td>856944637575090176</td>\n      <td>to my twenties: you brought me all the best an...</td>\n      <td>ENTJ</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['[CLS]', 'to', 'my', 'twenties', ':', 'you', ...</td>\n    </tr>\n    <tr>\n      <th>56783</th>\n      <td>856944637575090176</td>\n      <td>were right, it CAN kill a human being in unde...</td>\n      <td>ENTJ</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['[CLS]', 'were', 'right', ',', 'it', 'can', '...</td>\n    </tr>\n    <tr>\n      <th>61720</th>\n      <td>856944637575090176</td>\n      <td>only to teach you what absolutely doesn’t work...</td>\n      <td>ENTJ</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['[CLS]', 'only', 'to', 'teach', 'you', 'what'...</td>\n    </tr>\n    <tr>\n      <th>63004</th>\n      <td>856944637575090176</td>\n      <td>jace herondale.\\n\\nthat’s it. that’s the twee...</td>\n      <td>ENTJ</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['[CLS]', 'jace', 'heron', '##dale', '.', 'tha...</td>\n    </tr>\n    <tr>\n      <th>63251</th>\n      <td>856944637575090176</td>\n      <td>quel to #lotr. Set in the era of Aragorn's son...</td>\n      <td>ENTJ</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['[CLS]', 'que', '##l', 'to', '#', 'lot', '##r...</td>\n    </tr>\n    <tr>\n      <th>64260</th>\n      <td>856944637575090176</td>\n      <td>will show and six of you will be dead.” https:...</td>\n      <td>ENTJ</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['[CLS]', 'will', 'show', 'and', 'six', 'of', ...</td>\n    </tr>\n    <tr>\n      <th>69200</th>\n      <td>856944637575090176</td>\n      <td>utine. https://t.co/ZPGjLGmdhS@sarahcatstreet ...</td>\n      <td>ENTJ</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['[CLS]', 'ut', '##ine', '.', 'https', ':', '/...</td>\n    </tr>\n    <tr>\n      <th>72638</th>\n      <td>856944637575090176</td>\n      <td>s the same name as one of my children #ammteas...</td>\n      <td>ENTJ</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['[CLS]', 's', 'the', 'same', 'name', 'as', 'o...</td>\n    </tr>\n    <tr>\n      <th>73872</th>\n      <td>856944637575090176</td>\n      <td>o.  \\n\\n(And, hey, that's longer than my last ...</td>\n      <td>ENTJ</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['[CLS]', 'o', '.', '(', 'and', ',', 'hey', ',...</td>\n    </tr>\n    <tr>\n      <th>75666</th>\n      <td>856944637575090176</td>\n      <td>ink you all probably know this already but jus...</td>\n      <td>ENTJ</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['[CLS]', 'ink', 'you', 'all', 'probably', 'kn...</td>\n    </tr>\n    <tr>\n      <th>77253</th>\n      <td>856944637575090176</td>\n      <td>. https://t.co/XExDGWc32xnever stan real peopl...</td>\n      <td>ENTJ</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['[CLS]', '.', 'https', ':', '/', '/', 't', '....</td>\n    </tr>\n    <tr>\n      <th>77453</th>\n      <td>856944637575090176</td>\n      <td>to 2016 tumblr ever againWant blue check unde...</td>\n      <td>ENTJ</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['[CLS]', 'to', '2016', 'tu', '##mb', '##lr', ...</td>\n    </tr>\n    <tr>\n      <th>79255</th>\n      <td>856944637575090176</td>\n      <td>gothsI saw BWB’s ARC for the first time. I ope...</td>\n      <td>ENTJ</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['[CLS]', 'goth', '##si', 'saw', 'b', '##w', '...</td>\n    </tr>\n    <tr>\n      <th>84801</th>\n      <td>856944637575090176</td>\n      <td>.... no further notes on this for now just pro...</td>\n      <td>ENTJ</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['[CLS]', '.', '.', '.', '.', 'no', 'further',...</td>\n    </tr>\n    <tr>\n      <th>90797</th>\n      <td>856944637575090176</td>\n      <td>’s puppy had to have emergency surgery and the...</td>\n      <td>ENTJ</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['[CLS]', '’', 's', 'puppy', 'had', 'to', 'hav...</td>\n    </tr>\n    <tr>\n      <th>91206</th>\n      <td>856944637575090176</td>\n      <td>r holy shitmight do a newsletter this week wit...</td>\n      <td>ENTJ</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>['[CLS]', 'r', 'holy', 'shit', '##mi', '##ght'...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['liked_by']==856944637575090176]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "data": {
      "text/plain": "             liked_by                                               text  \\\n3  856944637575090176  me coming into 99+ tiktok notifs wondering wha...   \n\n   type  extravert  intuitive  thinking  judging  NT  SF  NF  ST  NJ  NP  SJ  \\\n3  ENTJ          1          1         1        1   1   0   0   0   1   0   0   \n\n   SP  \n3   0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>liked_by</th>\n      <th>text</th>\n      <th>type</th>\n      <th>extravert</th>\n      <th>intuitive</th>\n      <th>thinking</th>\n      <th>judging</th>\n      <th>NT</th>\n      <th>SF</th>\n      <th>NF</th>\n      <th>ST</th>\n      <th>NJ</th>\n      <th>NP</th>\n      <th>SJ</th>\n      <th>SP</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>856944637575090176</td>\n      <td>me coming into 99+ tiktok notifs wondering wha...</td>\n      <td>ENTJ</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_df[testing_df['liked_by']==856944637575090176]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]\n",
      "[1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "predict_person(testing_df.at[45, 'text'], testing_df.at[45, 'thinking'], 256)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      " \n",
      "1\n",
      "0\n",
      " \n",
      "1\n",
      "1\n",
      " \n",
      "1\n",
      "1\n",
      " \n",
      "1\n",
      "0\n",
      " \n",
      "1\n",
      "1\n",
      " \n",
      "1\n",
      "1\n",
      " \n",
      "1\n",
      "1\n",
      " \n",
      "0\n",
      "1\n",
      " \n",
      "1\n",
      "1\n",
      " \n",
      "1\n",
      "0\n",
      " \n",
      "1\n",
      "1\n",
      " \n",
      "1\n",
      "0\n",
      " \n",
      "1\n",
      "1\n",
      " \n",
      "1\n",
      "1\n",
      " \n",
      "1\n",
      "1\n",
      " \n",
      "1\n",
      "1\n",
      " \n",
      "1\n",
      "1\n",
      " \n",
      "0\n",
      "0\n",
      " \n",
      "0\n",
      "0\n",
      " \n",
      "1\n",
      "0\n",
      " \n",
      "1\n",
      "0\n",
      " \n",
      "1\n",
      "0\n",
      " \n",
      "1\n",
      "0\n",
      " \n",
      "1\n",
      "1\n",
      " \n",
      "0\n",
      "1\n",
      " \n",
      "0\n",
      "0\n",
      " \n",
      "1\n",
      "1\n",
      " \n",
      "1\n",
      "1\n",
      " \n",
      "1\n",
      "1\n",
      " \n",
      "1\n",
      "1\n",
      " \n",
      "1\n",
      "0\n",
      " \n",
      "1\n",
      "1\n",
      " \n",
      "1\n",
      "1\n",
      " \n",
      "0\n",
      "0\n",
      " \n",
      "0\n",
      "0\n",
      " \n",
      "1\n",
      "1\n",
      " \n",
      "1\n",
      "0\n",
      " \n",
      "1\n",
      "1\n",
      " \n",
      "1\n",
      "1\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for row in range(300,340):\n",
    "   print(predict_person(testing_df.at[row, 'text'], testing_df.at[row, 'intuitive'], 256))\n",
    "   print(test_df.at[row, 'intuitive'])\n",
    "   print(\" \")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Report the number of sentences.\n",
    "print('Number of persons in test set: {:,}\\n'.format(test_df.shape[0]))\n",
    "\n",
    "print('Positive samples: %d of %d (%.2f%%)' % (test_df.judging.sum(), len(test_df.judging), (test_df.judging.sum() / len(test_df.judging) * 100.0)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hIoseqqzt1pc"
   },
   "source": [
    "# from sklearn.metrics import matthews_corrcoef\n",
    "#\n",
    "# matthews_set = []\n",
    "#\n",
    "# # Evaluate each test batch using Matthew's correlation coefficient\n",
    "# print('Calculating Matthews Corr. Coef. for each batch...')\n",
    "#\n",
    "# # For each input batch...\n",
    "# for i in range(len(true_labels)):\n",
    "#\n",
    "#     # The predictions for this batch are a 2-column ndarray (one column for \"0\"\n",
    "#     # and one column for \"1\"). Pick the label with the highest value and turn this\n",
    "#     # in to a list of 0s and 1s.\n",
    "#     pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
    "#\n",
    "#     # Calculate and store the coef for this batch.\n",
    "#     matthews = matthews_corrcoef(true_labels[i], pred_labels_i)\n",
    "#     matthews_set.append(matthews)\n",
    "#"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_BreKUJ9t25l"
   },
   "source": [
    "#matthews_set"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "20s0Poipt6XT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
    "# flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "# flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "#\n",
    "# # Combine the correct labels for each batch into a single list.\n",
    "# flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "#\n",
    "# # Calculate the MCC\n",
    "# mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
    "#\n",
    "# print('MCC: %.3f' % mcc)\n",
    "\n",
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}